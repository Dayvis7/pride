
@book{keppler_carbon_2011,
	location = {Paris},
	title = {Carbon pricing, power markets and the competitiveness of nuclear power},
	isbn = {978-92-64-11887-4},
	series = {Nuclear development},
	pagetotal = {106},
	publisher = {Nuclear Energy Agency, Organisation for Economic Co-operation and Development},
	author = {Keppler, Jan Horst and Marcantonini, Claudio and {OECD} Nuclear Energy Agency and Organisation for Economic Co-operation \{and\} Development},
	date = {2011},
	langid = {english},
	keywords = {Electric power production, Emissions trading, Nuclear industry, {OECD} countries},
	file = {Keppler et al. - 2011 - Carbon pricing, power markets and the competitiven.pdf:/home/dotson/Zotero/storage/4CQ6FRPJ/Keppler et al. - 2011 - Carbon pricing, power markets and the competitiven.pdf:application/pdf}
}

@report{alsoenergy_university_2019,
	location = {Urbana, {IL}},
	title = {University of Illinois Solar Farm Dashboard},
	url = {http://s35695.mini.alsoenergy.com/Dashboard/2a5669735065572f4a42454b772b714d3d},
	shorttitle = {July 4 Metered Production},
	number = {2019-07-04},
	institution = {{AlsoEnergy}},
	type = {Web Dashboard},
	author = {{AlsoEnergy}},
	urldate = {2019-07-05},
	date = {2019-07-04},
	note = {http://s35695.mini.alsoenergy.com/Dashboard/2a5669735065572f4a42454b772b714d3d},
	file = {AlsoEnergy™ MiniSite:/home/dotson/Zotero/storage/MS3LD5XX/2a5669735065572f4a42454b772b714d3d.html:text/html}
}

@article{baker_optimal_2018,
	title = {Optimal sizing of flexible nuclear hybrid energy system components considering wind volatility},
	volume = {212},
	pages = {498--508},
	journaltitle = {Applied energy},
	author = {Baker, T. E. and Epiney, A. S. and Rabiti, C. and Shittu, E.},
	date = {2018},
	file = {Snapshot:/home/dotson/Zotero/storage/NVSMPGGX/S0306261917317762.html:text/html;2-RAVEN-windvolatility.pdf:/home/dotson/Zotero/storage/USXS3RZJ/2-RAVEN-windvolatility.pdf:application/pdf}
}

@article{fan_long-term_2020,
	title = {Long-term prediction of chaotic systems with machine learning},
	volume = {2},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.012080},
	doi = {10.1103/PhysRevResearch.2.012080},
	abstract = {Reservoir computing systems, a class of recurrent neural networks, have recently been exploited for model-free, data-based prediction of the state evolution of a variety of chaotic dynamical systems. The prediction horizon demonstrated has been about half dozen Lyapunov time. Is it possible to significantly extend the prediction time beyond what has been achieved so far? We articulate a scheme incorporating time-dependent but sparse data inputs into reservoir computing and demonstrate that such rare “updates” of the actual state practically enable an arbitrarily long prediction horizon for a variety of chaotic systems. A physical understanding based on the theory of temporal synchronization is developed.},
	pages = {012080},
	number = {1},
	journaltitle = {Physical Review Research},
	shortjournal = {Phys. Rev. Research},
	author = {Fan, Huawei and Jiang, Junjie and Zhang, Chun and Wang, Xingang and Lai, Ying-Cheng},
	urldate = {2020-06-23},
	date = {2020-03-30},
	note = {Publisher: American Physical Society},
	file = {Full Text PDF:/home/dotson/Zotero/storage/KN6XHGW4/Fan et al. - 2020 - Long-term prediction of chaotic systems with machi.pdf:application/pdf;APS Snapshot:/home/dotson/Zotero/storage/TET3XB5R/PhysRevResearch.2.html:text/html}
}

@report{isee_illinois_2015,
	location = {Urbana, {IL}},
	title = {Illinois Climate Action Plan ({iCAP})},
	url = {https://sustainability.illinois.edu/campus-sustainability/icap/},
	abstract = {This Plan represents a roadmap to a new, prosperous, and sustainable future for the University of Illinois at Urbana-Champaign campus. It outlines strategies, initiatives, and targets toward meeting the stated goal of carbon neutrality by 2050.},
	number = {2015},
	institution = {University of Illinois at Urbana-Champaign},
	type = {Full Report},
	author = {{iSEE}},
	urldate = {2019-07-03},
	date = {2015},
	langid = {american},
	file = {2015 - Illlinois Climate Action Plan.pdf:/home/dotson/Zotero/storage/XW9B3R3N/2015 - Illlinois Climate Action Plan.pdf:application/pdf;Institute for Sustainability, Energy, and Environment - 2015 - Illinois Climate Action Plan (iCAP).pdf:/home/dotson/Zotero/storage/DFFT4LEY/Institute for Sustainability, Energy, and Environment - 2015 - Illinois Climate Action Plan (iCAP).pdf:application/pdf;Snapshot:/home/dotson/Zotero/storage/EI6FVW2M/icap.html:text/html}
}

@inproceedings{marino_building_2016,
	title = {Building energy load forecasting using Deep Neural Networks},
	doi = {10.1109/IECON.2016.7793413},
	abstract = {Ensuring sustainability demands more efficient energy management with minimized energy wastage. Therefore, the power grid of the future should provide an unprecedented level of flexibility in energy management. To that end, intelligent decision making requires accurate predictions of future energy demand/load, both at aggregate and individual site level. Thus, energy load forecasting have received increased attention in the recent past. However, it has proven to be a difficult problem. This paper presents a novel energy load forecasting methodology based on Deep Neural Networks, specifically, Long Short Term Memory ({LSTM}) algorithms. The presented work investigates two {LSTM} based architectures: 1) standard {LSTM} and 2) {LSTM}-based Sequence to Sequence (S2S) architecture. Both methods were implemented on a benchmark data set of electricity consumption data from one residential customer. Both architectures were trained and tested on one hour and one-minute time-step resolution datasets. Experimental results showed that the standard {LSTM} failed at one-minute resolution data while performing well in one-hour resolution data. It was shown that S2S architecture performed well on both datasets. Further, it was shown that the presented methods produced comparable results with the other deep learning methods for energy forecasting in literature.},
	eventtitle = {{IECON} 2016 - 42nd Annual Conference of the {IEEE} Industrial Electronics Society},
	pages = {7046--7051},
	booktitle = {{IECON} 2016 - 42nd Annual Conference of the {IEEE} Industrial Electronics Society},
	author = {Marino, D. L. and Amarasinghe, K. and Manic, M.},
	date = {2016-10},
	keywords = {neural nets, decision making, Energy, Load forecasting, Computer architecture, Building Energy, building energy load forecasting, building management systems, Buildings, Deep Learning, deep neural network, Deep Neural Networks, electricity consumption data benchmark data set, Energy Load forecasting, energy management, energy management systems, energy wastage minimization, intelligent decision making, load forecasting, Load modeling, Logic gates, long short term memory algorithm, Long-Short-Term memory, {LSTM}, {LSTM}-based S2S architecture, {LSTM}-based sequence to sequence architecture, one-minute time-step resolution dataset, power engineering computing, power grid, power grids, Predictive models, Standards},
	file = {IEEE Xplore Abstract Record:/home/dotson/Zotero/storage/B37JLJIG/7793413.html:text/html;IEEE Xplore Full Text PDF:/home/dotson/Zotero/storage/PPFZKTHM/Marino et al. - 2016 - Building energy load forecasting using Deep Neural.pdf:application/pdf}
}

@article{pathak_model-free_2018,
	title = {Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	doi = {10.1103/PhysRevLett.120.024102},
	shorttitle = {Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data},
	abstract = {We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system’s past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.},
	pages = {024102},
	number = {2},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
	urldate = {2020-06-23},
	date = {2018-01-12},
	note = {Publisher: American Physical Society},
	file = {APS Snapshot:/home/dotson/Zotero/storage/ADCVZHYP/PhysRevLett.120.html:text/html;Accepted Version:/home/dotson/Zotero/storage/H7YJF5UY/Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Ch.pdf:application/pdf}
}

@article{vannitsem_predictability_2017,
	title = {Predictability of large-scale atmospheric motions: Lyapunov exponents and error dynamics},
	volume = {27},
	issn = {1054-1500, 1089-7682},
	url = {http://arxiv.org/abs/1703.04284},
	doi = {10.1063/1.4979042},
	shorttitle = {Predictability of large-scale atmospheric motions},
	abstract = {The deterministic equations describing the dynamics of the atmosphere (and of the climate system) are known to display the property of sensitivity to initial conditions. In the ergodic theory of chaos this property is usually quantified by computing the Lyapunov exponents. In this review, these quantifiers computed in a hierarchy of atmospheric models (coupled or not to an ocean) are analyzed, together with their local counterparts known as the local or finite-time Lyapunov exponents. It is shown in particular that the variability of the local Lyapunov exponents (corresponding to the dominant Lyapunov exponent) decreases when the model resolution increases. The dynamics of (finite-amplitude) initial condition errors in these models is also reviewed, and in general found to display a complicated growth far from the asymptotic estimates provided by the Lyapunov exponents. The implications of these results for operational (high resolution) atmospheric and climate modelling are also discussed.},
	pages = {032101},
	number = {3},
	journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	shortjournal = {Chaos},
	author = {Vannitsem, Stéphane},
	urldate = {2020-06-23},
	date = {2017-03},
	eprinttype = {arxiv},
	eprint = {1703.04284},
	keywords = {Nonlinear Sciences - Chaotic Dynamics},
	file = {arXiv Fulltext PDF:/home/dotson/Zotero/storage/QG8HSRZ3/Vannitsem - 2017 - Predictability of large-scale atmospheric motions.pdf:application/pdf;arXiv.org Snapshot:/home/dotson/Zotero/storage/UYFN4RN4/1703.html:text/html}
}

@article{vlachas_backpropagation_2020,
	title = {Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting of complex spatiotemporal dynamics},
	volume = {126},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608020300708},
	doi = {10.1016/j.neunet.2020.02.016},
	abstract = {We examine the efficiency of Recurrent Neural Networks in forecasting the spatiotemporal dynamics of high dimensional and reduced order complex systems using Reservoir Computing ({RC}) and Backpropagation through time ({BPTT}) for gated network architectures. We highlight advantages and limitations of each method and discuss their implementation for parallel computing architectures. We quantify the relative prediction accuracy of these algorithms for the long-term forecasting of chaotic systems using as benchmarks the Lorenz-96 and the Kuramoto–Sivashinsky ({KS}) equations. We find that, when the full state dynamics are available for training, {RC} outperforms {BPTT} approaches in terms of predictive performance and in capturing of the long-term statistics, while at the same time requiring much less training time. However, in the case of reduced order data, large scale {RC} models can be unstable and more likely than the {BPTT} algorithms to diverge. In contrast, {RNNs} trained via {BPTT} show superior forecasting abilities and capture well the dynamics of reduced order systems. Furthermore, the present study quantifies for the first time the Lyapunov Spectrum of the {KS} equation with {BPTT}, achieving similar accuracy as {RC}. This study establishes that {RNNs} are a potent computational framework for the learning and forecasting of complex spatiotemporal systems.},
	pages = {191--217},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Vlachas, P. R. and Pathak, J. and Hunt, B. R. and Sapsis, T. P. and Girvan, M. and Ott, E. and Koumoutsakos, P.},
	urldate = {2020-06-23},
	date = {2020-06-01},
	langid = {english},
	keywords = {Complex systems, Kuramoto–Sivashinsky, Lorenz-96, Reservoir Computing, {RNN}, {LSTM}, {GRU}, Time series forecasting},
	file = {ScienceDirect Snapshot:/home/dotson/Zotero/storage/LWZ2SCR7/S0893608020300708.html:text/html;ScienceDirect Full Text PDF:/home/dotson/Zotero/storage/74GMJDKK/Vlachas et al. - 2020 - Backpropagation algorithms and Reservoir Computing.pdf:application/pdf}
}

@article{wikner_combining_2020,
	title = {Combining machine learning with knowledge-based modeling for scalable forecasting and subgrid-scale closure of large, complex, spatiotemporal systems},
	volume = {30},
	issn = {1054-1500},
	url = {https://aip-scitation-org.proxy2.library.illinois.edu/doi/10.1063/5.0005541},
	doi = {10.1063/5.0005541},
	abstract = {We consider the commonly encountered situation (e.g., in weather forecast) where the goal is to predict the time evolution of a large, spatiotemporally chaotic dynamical system when we have access to both time series data of previous system states and an imperfect model of the full system dynamics. Specifically, we attempt to utilize machine learning as the essential tool for integrating the use of past data into predictions. In order to facilitate scalability to the common scenario of interest where the spatiotemporally chaotic system is very large and complex, we propose combining two approaches: (i) a parallel machine learning prediction scheme and (ii) a hybrid technique for a composite prediction system composed of a knowledge-based component and a machine learning-based component. We demonstrate that not only can this method combining (i) and (ii) be scaled to give excellent performance for very large systems but also that the length of time series data needed to train our multiple, parallel machine learning components is dramatically less than that necessary without parallelization. Furthermore, considering cases where computational realization of the knowledge-based component does not resolve subgrid-scale processes, our scheme is able to use training data to incorporate the effect of the unresolved short-scale dynamics upon the resolved longer-scale dynamics (subgrid-scale closure).},
	pages = {053111},
	number = {5},
	journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	shortjournal = {Chaos},
	author = {Wikner, Alexander and Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Arcomano, Troy and Szunyogh, Istvan and Pomerance, Andrew and Ott, Edward},
	urldate = {2020-06-23},
	date = {2020-05-01},
	note = {Publisher: American Institute of Physics},
	file = {Full Text PDF:/home/dotson/Zotero/storage/MLHBQFE6/Wikner et al. - 2020 - Combining machine learning with knowledge-based mo.pdf:application/pdf;Snapshot:/home/dotson/Zotero/storage/PJWQ3DMT/5.html:text/html}
}

@inproceedings{rykhlevskii_impact_2019,
	location = {Washington, {DC}, United States},
	title = {The Impact of Xenon-135 on Load Following Transatomic Power Molten Salt Reactor},
	eventtitle = {American Nuclear Society Winter Conference},
	booktitle = {Transactions of the American Nuclear Society},
	publisher = {American Nuclear Society},
	author = {Rykhlevskii, Andrei and O'Grady, Daniel and Kozlowski, Tomasz and Huff, Kathryn D.},
	date = {2019-11-17},
	file = {Rykhlevskii et al. - 2019 - The Impact of Xenon-135 on Load Following Transato.pdf:/home/dotson/Zotero/storage/AAVT2MDT/Rykhlevskii et al. - 2019 - The Impact of Xenon-135 on Load Following Transato.pdf:application/pdf}
}

@article{bianchi_reservoir_2020,
	title = {Reservoir computing approaches for representation and classification of multivariate time series},
	url = {http://arxiv.org/abs/1803.07870},
	abstract = {Classification of multivariate time series ({MTS}) has been tackled with a large variety of methodologies and applied to a wide range of scenarios. Reservoir Computing ({RC}) provides efficient tools to generate a vectorial, fixed-size representation of the {MTS} that can be further processed by standard classifiers. Despite their unrivaled training speed, {MTS} classifiers based on a standard {RC} architecture fail to achieve the same accuracy of fully trainable neural networks. In this paper we introduce the reservoir model space, an unsupervised approach based on {RC} to learn vectorial representations of {MTS}. Each {MTS} is encoded within the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics. Compared to other {RC} methods, our model space yields better representations and attains comparable computational performance, thanks to an intermediate dimensionality reduction procedure. As a second contribution we propose a modular {RC} framework for {MTS} classification, with an associated open-source Python library. The framework provides different modules to seamlessly implement advanced {RC} architectures. The architectures are compared to other {MTS} classifiers, including deep learning models and time series kernels. Results obtained on benchmark and real-world {MTS} datasets show that {RC} classifiers are dramatically faster and, when implemented using our proposed representation, also achieve superior classification accuracy.},
	journaltitle = {{arXiv}:1803.07870 [cs]},
	author = {Bianchi, Filippo Maria and Scardapane, Simone and Løkse, Sigurd and Jenssen, Robert},
	urldate = {2020-06-23},
	date = {2020-06-07},
	eprinttype = {arxiv},
	eprint = {1803.07870},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/dotson/Zotero/storage/NFUWIT4A/Bianchi et al. - 2020 - Reservoir computing approaches for representation .pdf:application/pdf;arXiv.org Snapshot:/home/dotson/Zotero/storage/8F5JL3G3/1803.html:text/html}
}

@article{lukosevicius_reservoir_2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	volume = {3},
	issn = {1574-0137},
	url = {http://www.sciencedirect.com/science/article/pii/S1574013709000173},
	doi = {10.1016/j.cosrev.2009.03.005},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network ({RNN}) training, where an {RNN} (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of {RNNs} and outperformed classical fully trained {RNNs} in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.},
	pages = {127--149},
	number = {3},
	journaltitle = {Computer Science Review},
	shortjournal = {Computer Science Review},
	author = {Lukoševičius, Mantas and Jaeger, Herbert},
	urldate = {2020-06-23},
	date = {2009-08-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/dotson/Zotero/storage/DWKPNPMV/S1574013709000173.html:text/html;ScienceDirect Snapshot:/home/dotson/Zotero/storage/4VIEIGQE/S1574013709000173.html:text/html;ScienceDirect Full Text PDF:/home/dotson/Zotero/storage/4P3YJ6FK/Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf:application/pdf}
}

@article{gallicchio_deep_2019,
	title = {Deep Echo State Network ({DeepESN}): A Brief Survey},
	url = {http://arxiv.org/abs/1712.04323},
	shorttitle = {Deep Echo State Network ({DeepESN})},
	abstract = {The study of deep recurrent neural networks ({RNNs}) and, in particular, of deep Reservoir Computing ({RC}) is gaining an increasing research attention in the neural networks community. The recently introduced Deep Echo State Network ({DeepESN}) model opened the way to an extremely efficient approach for designing deep neural networks for temporal data. At the same time, the study of {DeepESNs} allowed to shed light on the intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers, i.e. on the bias of depth in {RNNs} architectural design. In this paper, we summarize the advancements in the development, analysis and applications of {DeepESNs}.},
	journaltitle = {{arXiv}:1712.04323 [cs, stat]},
	author = {Gallicchio, Claudio and Micheli, Alessio},
	urldate = {2020-06-23},
	date = {2019-02-25},
	eprinttype = {arxiv},
	eprint = {1712.04323},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/dotson/Zotero/storage/FPRLPH7Q/Gallicchio and Micheli - 2019 - Deep Echo State Network (DeepESN) A Brief Survey.pdf:application/pdf;arXiv.org Snapshot:/home/dotson/Zotero/storage/N7LAGAHF/1712.html:text/html}
}

@article{chouikhi_genesis_2018,
	title = {Genesis of Basic and Multi-Layer Echo State Network Recurrent Autoencoders for Efficient Data Representations},
	url = {http://arxiv.org/abs/1804.08996},
	abstract = {It is a widely accepted fact that data representations intervene noticeably in machine learning tools. The more they are well defined the better the performance results are. Feature extraction-based methods such as autoencoders are conceived for finding more accurate data representations from the original ones. They efficiently perform on a specific task in terms of 1) high accuracy, 2) large short term memory and 3) low execution time. Echo State Network ({ESN}) is a recent specific kind of Recurrent Neural Network which presents very rich dynamics thanks to its reservoir-based hidden layer. It is widely used in dealing with complex non-linear problems and it has outperformed classical approaches in a number of tasks including regression, classification, etc. In this paper, the noticeable dynamism and the large memory provided by {ESN} and the strength of Autoencoders in feature extraction are gathered within an {ESN} Recurrent Autoencoder ({ESN}-{RAE}). In order to bring up sturdier alternative to conventional reservoir-based networks, not only single layer basic {ESN} is used as an autoencoder, but also Multi-Layer {ESN} ({ML}-{ESN}-{RAE}). The new features, once extracted from {ESN}'s hidden layer, are applied to classification tasks. The classification rates rise considerably compared to those obtained when applying the original data features. An accuracy-based comparison is performed between the proposed recurrent {AEs} and two variants of an {ELM} feed-forward {AEs} (Basic and {ML}) in both of noise free and noisy environments. The empirical study reveals the main contribution of recurrent connections in improving the classification performance results.},
	journaltitle = {{arXiv}:1804.08996 [cs]},
	author = {Chouikhi, Naima and Ammar, Boudour and Alimi, Adel M.},
	urldate = {2020-06-23},
	date = {2018-06-09},
	eprinttype = {arxiv},
	eprint = {1804.08996},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/dotson/Zotero/storage/34A62833/Chouikhi et al. - 2018 - Genesis of Basic and Multi-Layer Echo State Networ.pdf:application/pdf;arXiv.org Snapshot:/home/dotson/Zotero/storage/3Y4U4VQK/1804.html:text/html}
}

@article{verzelli_echo_2019,
	title = {Echo State Networks with Self-Normalizing Activations on the Hyper-Sphere},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-50158-4},
	doi = {10.1038/s41598-019-50158-4},
	abstract = {Among the various architectures of Recurrent Neural Networks, Echo State Networks ({ESNs}) emerged due to their simplified and inexpensive training procedure. These networks are known to be sensitive to the setting of hyper-parameters, which critically affect their behavior. Results show that their performance is usually maximized in a narrow region of hyper-parameter space called edge of criticality. Finding such a region requires searching in hyper-parameter space in a sensible way: hyper-parameter configurations marginally outside such a region might yield networks exhibiting fully developed chaos, hence producing unreliable computations. The performance gain due to optimizing hyper-parameters can be studied by considering the memory–nonlinearity trade-off, i.e., the fact that increasing the nonlinear behavior of the network degrades its ability to remember past inputs, and vice-versa. In this paper, we propose a model of {ESNs} that eliminates critical dependence on hyper-parameters, resulting in networks that provably cannot enter a chaotic regime and, at the same time, denotes nonlinear behavior in phase space characterized by a large memory of past inputs, comparable to the one of linear networks. Our contribution is supported by experiments corroborating our theoretical findings, showing that the proposed model displays dynamics that are rich-enough to approximate many common nonlinear systems used for benchmarking.},
	pages = {13887},
	number = {1},
	journaltitle = {Scientific Reports},
	author = {Verzelli, Pietro and Alippi, Cesare and Livi, Lorenzo},
	urldate = {2020-06-24},
	date = {2019-09-25},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/home/dotson/Zotero/storage/VUP8QT4W/Verzelli et al. - 2019 - Echo State Networks with Self-Normalizing Activati.pdf:application/pdf;Snapshot:/home/dotson/Zotero/storage/79Q2E8LG/s41598-019-50158-4.html:text/html}
}

@article{ma_deepr-esn_2020,
	title = {{DeePr}-{ESN}: A deep projection-encoding echo-state network},
	volume = {511},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025519309053},
	doi = {10.1016/j.ins.2019.09.049},
	shorttitle = {{DeePr}-{ESN}},
	abstract = {As a recurrent neural network that requires no training, the reservoir computing ({RC}) model has attracted widespread attention in the last decade, especially in the context of time series prediction. However, most time series have a multiscale structure, which a single-hidden-layer {RC} model may have diﬃculty capturing. In this paper, we propose a novel multiple projection-encoding hierarchical reservoir computing framework called Deep Projection-encoding Echo State Network ({DeePr}-{ESN}). The most distinctive feature of our model is its ability to learn multiscale dynamics through stacked {ESNs}, connected via subspace projections. Speciﬁcally, when an input time series is projected into the highdimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., an autoencoder or {PCA}) projects the echo-state representations into a lower-dimensional feature space. These representations are the principal components of the echo-state representations, which removes the high frequency components of the representations. These can then be processed by another {ESN} through random connections. By using projection layers and encoding layers alternately, our {DeePr}-{ESN} can provide much more robust generalization performance than previous methods, and also fully takes advantage of the temporal kernel property of {ESNs} to encode the multiscale dynamics of time series. In our experiments, the {DeePr}-{ESNs} outperform both standard {ESNs} and existing hierarchical reservoir computing models on some artiﬁcial and real-world time series prediction tasks.},
	pages = {152--171},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Ma, Qianli and Shen, Lifeng and Cottrell, Garrison W.},
	urldate = {2020-06-24},
	date = {2020-02},
	langid = {english},
	file = {Ma et al. - 2020 - DeePr-ESN A deep projection-encoding echo-state n.pdf:/home/dotson/Zotero/storage/FXM6TRPX/Ma et al. - 2020 - DeePr-ESN A deep projection-encoding echo-state n.pdf:application/pdf}
}

@incollection{lukosevicius_practical_2012,
	location = {Berlin, Heidelberg},
	title = {A Practical Guide to Applying Echo State Networks},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_36},
	series = {Lecture Notes in Computer Science},
	abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network ({ESN}) is one of the key reservoir computing “flavors”. While being practical, conceptually simple, and easy to implement, {ESNs} require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying {ESNs}, as well as some more advanced application-specific modifications.},
	pages = {659--686},
	booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
	publisher = {Springer},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	urldate = {2020-06-24},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-35289-8_36},
	keywords = {Little Mean Square, Output Feedback, Recurrent Neural Network, Ridge Regression, Spectral Radius},
	file = {Springer Full Text PDF:/home/dotson/Zotero/storage/HZVQFIJD/Lukoševičius - 2012 - A Practical Guide to Applying Echo State Networks.pdf:application/pdf}
}

@article{li_temporalspatial_2011,
	title = {Temporal–Spatial Distribution of Atmospheric Predictability Limit by Local Dynamical Analogs},
	volume = {139},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/mwr/article/139/10/3265/71306/Temporal-Spatial-Distribution-of-Atmospheric},
	doi = {10.1175/MWR-D-10-05020.1},
	pages = {3265--3283},
	number = {10},
	journaltitle = {Monthly Weather Review},
	shortjournal = {Mon. Wea. Rev.},
	author = {Li, Jianping and Ding, Ruiqiang},
	urldate = {2020-06-24},
	date = {2011-10-01},
	langid = {english},
	note = {Publisher: American Meteorological Society},
	file = {Full Text PDF:/home/dotson/Zotero/storage/EGN4TQZF/Li and Ding - 2011 - Temporal–Spatial Distribution of Atmospheric Predi.pdf:application/pdf;Snapshot:/home/dotson/Zotero/storage/N5VUGZPZ/Temporal-Spatial-Distribution-of-Atmospheric.html:text/html}
}

@article{jaeger_harnessing_2004,
	title = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
	volume = {304},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1091277},
	doi = {10.1126/science.1091277},
	shorttitle = {Harnessing Nonlinearity},
	pages = {78--80},
	number = {5667},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Jaeger, H.},
	urldate = {2020-06-24},
	date = {2004-04-02},
	langid = {english},
	file = {Jaeger - 2004 - Harnessing Nonlinearity Predicting Chaotic System.pdf:/home/dotson/Zotero/storage/95LH9UQN/Jaeger - 2004 - Harnessing Nonlinearity Predicting Chaotic System.pdf:application/pdf}
}

@software{korndorfer_pyesn_2015,
	title = {{pyESN}},
	url = {https://github.com/cknd/pyESN},
	author = {Korndörfer, C},
	urldate = {2020-06-25},
	date = {2015},
	file = {Snapshot:/home/dotson/Zotero/storage/SNJKXADH/pyESN.html:text/html}
}

@unpublished{dotson_optimal_2020,
	location = {Online Virtual Meeting},
	title = {Optimal Sizing of a Micro-reactor for Embedded Grid Systems},
	abstract = {In 2015, the University of Illinois at Urbana-Champaign ({UIUC}) made a commitment to reach carbon neutrality by 2050 in the Illinois Climate Action Plan ({iCAP}) 
This plan identified nuclear energy as a potential contributor to this goal.
In this work, we use the Temoa framework to find the optimal size for a micro-reactor that would minimize the levelized cost of electricity ({LCOE}) for the {UIUC} embedded grid.
The greatest source of economic variability is contained in the capital costs of a micro-reactor. Thus we examine both scenarios where the reactor is provided at no cost to the university and purchased at full price.},
	type = {Technical Presentation},
	howpublished = {Technical Presentation},
	note = {American Nuclear Society Annual Meeting 2020},
	author = {Dotson, Samuel G.},
	date = {2020-06-10},
	langid = {english},
	file = {Dotson - 2020 - Optimal Sizing of a Micro-reactor for Embedded Gri.pdf:/home/dotson/Zotero/storage/P4QU7Q2Y/Dotson - 2020 - Optimal Sizing of a Micro-reactor for Embedded Gri.pdf:application/pdf}
}